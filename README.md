# Highload Instagram

> [Методические указания](https://github.com/init/highload/blob/main/homework_architecture.md)

## Тема и целевая аудитория

**Instagram** — американская социальная сеть для обмена фотографиями и видео, основанная Кевином Систромом и Майком Кригером.

### Факты про целевую аудиторию

Целевой аудиторией являются люди, имеющие отношение к бизнесу, большие компании, общеизвестные деятели и просто люди, которые хотят делиться своей жизнью в социальных сетях.

- В среднем количество активных пользователей достигает 2-х биллионов;
- Пользователи проводят в среднем 11 часов в месяц в соц. сети;
- 30.8% аудитории - люди от 18 до 24 лет, а  30.3% - люди от 25 до 34 года.

Страны лидеры по размеру аудитории на момент января 2023 года:

| Страна      | Пользователей в год, млн. |
| ----------- | ------------------------- |
| Индия       | 229.25                    |
| США         | 143.35                    |
| Бразилия    | 143.35                    |
| Индонезия   | 89.15                     |
| Турция      | 48.65                     |
| Япония      | 45.7                      |
| Мексика     | 36.7                      |

Распределние пользователей за последний месяц (Январь 2023):

| Страна                | Пользователей в месяц, млн. | Пользователей в месяц, %  |
| --------------------- | --------------------------- | ------------------------- |
| CША                   | 435                         | 16.65                     |
| Индия                 | 403.7                       | 10.90                     |
| Бразилия              | 200.2                       | 10.22                     |
| Турция                | 162.4                       | 3.55                      |
| Индонезия             | 161.5                       | 3.50                      |
| Остальные страны      | 1 500                       | 55.18                     |

### Минимально жизнеспособный продукт

1. Создание, редактирование, просмотр профиля;
2. Публикация фото и видео с продолжительностью не более 60 секунд;
3. Возможность поставить лайк и оставить комментарий к посту;
4. Возможность подписаться на пользователя;
5. Возможность просматривать ленту с подписками.

## Расчет нагрузки

### Продуктовые метрики

1. Месячная аудитория: 2 биллиона [[1]](<https://www.statista.com/topics/1882/instagram>);
2. Дневная аудитроия: 500 миллионов [[2]](<https://www.businessdit.com/time-spend-instagram-statistics/>);
3. Среднее время онлайна пользователя - 3 мин. 6 сек [[2]](<https://www.businessdit.com/time-spend-instagram-statistics/>). За это время пользователю удается просмотреть 10 страниц.

#### Авторизация

Так как информацию насчет среднего количества входов в аккаунт и регистраций найти не удалось, прибегну к приблизительным расчетам.
Будем считать, что так как в среднем за день имеется 500 млн. активных пользователей, то и авторизаций производится столько же.

#### Регистрация

Перейдем к регистрации: за 2021 год было насчитано 1.28 биллиона пользователей, когда за 2022 1.32 биллиона [[3]](<https://www.statista.com/statistics/183585/instagram-number-of-global-users/>). Это значит, что за год прибавилось 40 000 000 пользователей. Значит, грубой оценкой можно посчитать среднее количество регистраций в день: `4 * 10^7 / 365 = 108 089`. То есть количество регистраций в день достигает 108 тыс.

#### Лайки и комментарии

Информация о лайках и комментариях была найдена в источнике [[4]](<https://mention.com/en/blog/instagram-statistics-report/>):

- Среднее количество лайков, оставляемых под постом: 1 261, но всего среднее кол-во лайков в день 2 биллиона;
- Среднее количество комментариев, оставляемых под постом: 24.5, но всего среднее кол-во комментариев в день 38 миллиона;

#### Лента

Считаем в среднем: имеем число пользователей в день, теперь считаем каджого. Основываясь на статистике о среднем времени (53 минуты), проведенным пользователем на сервисе [[5]](<https://www.businessdit.com/time-spend-instagram-statistics/>), будем предполагать, что большую часть времени он проводит за скроллингом ленты (80%).

Далее нужно понять, сколько времени занимет у пользователя на один пост (просмотр, чтение описания, лайк, комментирование). Мозг может обрабатывать изображение, которое глаз видел только в течение 13 миллисекунд, но человек не сможет сознательно регистрировать эти кадры, поэтому возьмем 2-5 секунды на просмотр фото поста или видео - длина видео (в среднем 10 секунд) и еще 2-5 на проставление лайка и просмотра/написания комментариев. Получаем, что на один пост пользователь будет тратить:

> `Формула`: [время пользователя за скроллиногом ленты] / [время на один пост]\
> 0.7 x [(0.8 x 53 x 60) / 10] + 0.3 x [(0.8 x 53 x 60) / 15] = 228 постов в день на пользователя

Стоит учитывать, что посты с фото и видео относятся как 7:3. Посты будут загружаться каждые 30 постов. Тогда для одного пользователя всего будет `228 / 30 = 76` загрузок ленты на пользователя в день. Для всех пользователей:

> 76 x 500 000 000 = 38 000 000 000

#### Сводная таблица по среднему количеству действий пользователя по типам в день

| Тип запроса               | Среднее количество действий (в день) |
| ------------------------- | ------------------------------------ |
| Регистрация               | 219 тыс.                             |
| Авторизация               | 500 млн.                             |
| Публикация фото / видео   | 95 млн.                              |
| Проставление лайков       | 2 000 млн.                           |
| Проставление комментариев | 38 млн.                              |
| Просмотр ленты            | 38 000 млн.                          |

### Технические метрики

#### Размер хранения в разбивке по типам данных

- Информация о пользователе

    У пользователя будут следующие поля:

  - `Имя, Фамилия, Никнейм`: все по 32 символа, каждый из которых весит байт;
  - `Почта, Дата рождения`: 4 байта;
  - `Пароль`: в захэшированном виде 32 байта;
  - `Описание`: 64 симовла по байту каждый;
  - `Аватарка`: 50 Кбайт.

  На одного пользователя:

  > 30 + 30 + 30 + 4 x 125 000 + 4 + 4 + 50 000 = 550 098 байт = 4 Мб`

  На всех пользователей понадобится:

  > 2 000 000 000 x 4 = 8 * 10^9 Мб = 8 000 Тб ~= 8 Пб.

- Размер постов

    Начну с размера поста с *фотографией*: средний размер исходной фотографии составляет около 2 Мб. Однако, для хранения на серверах и передачи через сеть, изображение будет сжиматься с использованием алгоритмов оптимизации и сжатия. В результате, средний вес сжатой картинки на платформе составит примерно **300 Кб**.

    Размер *видео* будет зависеть от многих характеристик, но будем брать грубую оценку в среднем. Отталкиваемся от битрейта видео - определяет количество бит, передаваемых в секунду и влияет на качество и размер видеофайла. Обычно на Instagram рекомендуется использовать битрейт от 2 до 5 Mbps [[6]](<https://www.puckermob.com/tech/everything-you-wanted-to-know-about-best-bitrate-for-instagram/>). Для примера возьмем средний битрейт видео в 3 Mbps. Расчет объема данных: для определения размера видеофайла в байтах нужно умножить средний битрейт на длительность видео (в секундах) и поделить на 8 для преобразования из мегабитов в байты.

    > Размер видео (в байтах) = (Средний битрейт x Длительность видео) / 8

    В нашем случае, длительность видео 10 секунд, а средний битрейт 3 Mbps:

    > (3 Mbps x 10 сек) / 8 = 30 Mbps / 8 = 3.75 Мб

    Размер видео (в байтах) = **3.75 Мб**

    Описание к посту займет около 150 байт (возьмем в среднем 150 символов по байту). Итого, пост с картинкой будет весить 300 Кб. В случае с видео - возьмем видео в 10 секунд, которое будет весить 3.75 Мб и пост будет уже весить 3.78 Мб.

    С момента запуска приложения было насчитано около 50 миллиардов фотографий [[7]](<https://earthweb.com/how-many-pictures-are-on-instagram/>), в секунду загружается 1 074 фотографий. Значит, возьмем 3/4 постов с фотографией и 1/4 с видео и посчитаем приблизительный суммарный размер базы, которая будет хранить все посты:

    > 50 000 000 000 x (0.75 x 0.3 + 0.25 x 3.78) = 5 850 000 000 Мб = 58 500 Тб = 58.5 Пб

- Размер лайков

    Для лайков будут учитываться два целочисленных значения по 8 байт каждый - id пользователя, отсавлющий лайк и id поста, на который ставится лайк. При 50 биллионов постов предположим, среднее количество лайков на пост - 1 261. Тогда произведем расчет:

    > 50 000 000 000 x 1 261 x 16 байт = 800 Тб

- Размер комментариев

    Так как комментарии оставляются реже в отличие от лайков, предположим, что в среднем каждый пост будет иметь по 50 комментариев. В состав комментария будут входить id комментария, id пользователя и поста по 8 байт, содержимое - 1 Кбайт, дата создания 4 байта. Получаем следующее значение:

    > 50 000 000 000 x 50 x 1024 = 2 560 Тб

Итого сводная таблица по выделяемому месту для существенных блоков данных:

| Название данных | Количество, шт. | Место, Тб.  |
|-----------------|-----------------|-------------|
| Посты           | 50 000 млн.     | 58 500      |
| Пользователи    | 2 000 млн.      | 8 000       |
| Лайки           | 5 000 блн.      | 800         |
| Комментарии     | 2 500 блн.      | 2 560       |

#### Сетевой трафик

- Взглянем на авторизацию и регистрацию и посчитаем нагрузку:

    > 500 219 000 (в день) x 4 Мб / (24 * 3600) = 11 388 Мб/с = 88 Гбит/с

    Для пиковой нагрузки умножу показатель в 2 раза:

    > 88 Гбит/с x 2 = 176 Гб/с

- Рассмотрим загрузку поста. Зная, что за секунду загружается 1 074 постов, посчитаем нагрузку на сеть:

    > 1 074 x (0.75 x 2.2 + 0.25 x 8.2) = 3 973.8 Мб/с = 31 Гбит/с

    Для пиковой нагрузки умножу показатель в 2 раза:

    > 31 Гбит/с x 2 = 62 Гб/с

- Теперь посчитаем загрузку ленты. Ссылаясь на проделанные расчеты выше, получим:

    > [38 000 000 000 (в день) x 30 (лимит постов для одной загрузки ленты) x (0.7 x 0.3 + 0.3 x 3.78)] / (24 x 3 600) = 1 773 333 Мб/с = 1 773 Гбит/с

    Для пиковой нагрузки умножу показатель в 2 раза:

    > 1 773 Гбит/с x 2 = 3 546 Гб/с

Так как остальные запросы будут не настолько дорогие относительно этих двух, заложу на них 5 Гбит/с.

Получу итоговую сетевую нагрузку:

> 88 + 31 + 1 773 = 1 892 Гбит/с

| Тип данных                | Сетевая нагрузка, Гб/с | Пиковая сетевая нагрузка, Гб/с  |
|---------------------------|------------------------|---------------------------------|
| Авторизация & Регистрация | 88                     | 176                             |
| Загрузка поста            | 31                     | 62                              |
| Лента                     | 1 773                  | 3 546                          |

Суточная нагрузка на сеть - `1 892 Гб/с * 3600 * 24 = 163 468 800 Гб/сутки`

### RPS в разбивке по типам запросов

Уже по полученным данным посчитаем кол-во запросов в секунду в среднем:

- Регистрация/Авторизация:   `500 000 000 / (24 * 3600) = 5 787 rps`
- Публикация фото / видео:   `95 000 000 / (24 * 3600) = 1 099 rps`
- Проставление лайков:       `2 000 000 000 / (24 * 3600) = 23 148 rps`
- Добавление комментариев:   `38 000 000 / (24 * 3600) = 439 rps`
- Просмотр ленты:            `38 000 000 000 / (24 * 3600) = 439 814 rps`

Теперь нужно нужно посчитать пиковую нагрузку:

Для авторизации можно умножить среднюю нагрузку на 2, как и с публикацией фото и видео и просмотром ленты. Это объясняется наибольшей активностью пользователей в определенный момент времени.

В случае лайков с комментариями надо умножать среднюю нагрузку на 4. Это обуславливается выкладыванием известной личности какой-либо фотографии - ее начнут активно лайкать и комментировать.

| Тип запроса               | Средняя нагрузка, rps                | Пиковая нагрузка, rps |
| ------------------------- | ------------------------------------ | --------------------- |
| Авторизация               | 5 787                                | 11 574                |
| Публикайия фото / видео   | 1 099                                | 2 198                 |
| Проставление лайков       | 23 148                               | 92 592                |
| Добавление комментариев   | 439                                  | 1756                  |
| Просмотр ленты            | 439 814                              | 879 628               |
| **Итого**                 | **470 287**                          | **987 748**           |

## Логическая схема

![Логическая схема](/img/logical_schema.png)

### Описание таблиц

1. Пользователи (users) - таблица со всеми необходимыми данными о пользователе.
    - Primary key: id пользователяj
2. Посты (posts) - таблица с постами пользователями, которая включает в себя все необходимое для отображения поста.
    - Primary key: id поста;
    - Foreign key: user_id.
3. Лайки (likes) - таблица с информацией, что i-й пользователь поставил лайк j-му посту.
    - Foreign key: user_id, post_id.
4. Комментарии (comments) - таблица с информацией, что i-й пользователь поставил лайк j-му посту с непустым сообщением.
    - Foreign key: user_id, post_id.
5. Подписки (subscriptions) - таблица с информацией о том, что i-й пользователь подписан на j-го.
    - Foreign key: creator_id, follower_id.
6. Сессии (sessions) - таблица со всеми сессиями пользователей.
    - Foreign key: user_id.

## Физическая схема

 ![Физическая схема](/img/physical_schema.png)

### Выбор БД

1. Сессии пользователей

    Сессии пользователей будут очень часто изменяться по очевидным причиам. Из-за этого нужно осуществить быстрый доступ на получение текущих пользователей, быстрое удаление и добавление. Таким образом их лучше всего хранить в оперативной памяти, с чем прекрасно будет справляться нереляцонная in-memory база данных - `Redis`. В данном случае, ключом будет значение сессии, а значением - id пользователя и дата истечения сессии.

2. Пользователи и посты

    Пользователи же будут находиться в реляционной базе `PostgreSQL`, так как она представляет возможность делать сложные запросы, устанавливать отношения для легкой организации и связывания данных, а также поддерживает обширный функционал для масштабирования. Однако аватарки пользователей хранить там же не представляется возможным за счет большого размера картинок всех пользователей, что будет заметно замедлять работу базы. Поэтому для хранения аватарок будет использоваться `S3-хранилище`, который имеет высокую скорость чтения, высокую доступность и долговечных данных. В самой же реляционной базе будем хранить ссылку на картунку в S3.

    Аналогично пользователям данные постов будем хранить в `PostgreSQL`, а картинки/видео в `S3-хранилище`.

3. Комментарии, лайки, подписки

    Данные хорошо ложаться на реляционную модель, поэтому будем хранить их в `PostgreSQL`.

### Индексы

- Индекс на поле `nickname` в таблице `users` будет нужен в случае того, если пользователь решит найти другого по никнейму. Таким образом индекс увеличит селективность по этому полю;
- Индекс на поле `user_id` в таблице `posts` нужен для быстрого поиска пользователя, пост которого был увиден;
- Индекс на поле `post_id` в таблице `comments` нужен для быстрой подгрузки всех комментраиев у посту;
- Индексы на поля `creator_id` и `follower_id` в таблице `subscriptions` будут полезны для улучшения селективности подписчиков и подписок соответственно.

### Шардирование

- Users: шардирование будет проводиться по range-based принципу для более равномерного распределения данных;
- Posts: здесь можно применить партиционирование по принципу range-based, а шардировать по разным инстансам по дате создания поста. Таким образом новые посты будут находиться в одном месте, что позволит быстро получать их для прогрузкки ленты или первых постов на странице пользователя. Старые же посты так же будут доставляться быстро;
- Comments: комментарии будет логично шардировать по такому же принципу, что и посты для согласованности данных, то есть на том же инстансе, что и посты нужной даты.

### Репликация

Для PostgreSQL будет сделана Master-Slaves репликация. Таким образом, когда на Master базу будут записываться данные, с Slave баз будет читаться информация. Также при отказе одного Slave узла чтение может продолжится посредством переключения на другой узел.

## Технологии

| Технология    | Область применения   | Мотивация      |
|-------------- | -------------------- | -------------- |
| Golang        | Backend dev          | Популярный, быстрый, читаемый, поддерживающий многопоточную и асинхронную работу язык. За счет своей опулярности имеет множество готовых решений для backend-разработки |
| TS / React    | Frontend dev         | TS за счет своей типизации позволит писать более понятный и читаемый код. React фреймворк имеет множество инструментов для эффективной разработки |
| Swift / Kotlin| Mobile   dev         | В силу того, что приложение имеет большую популярность в мобильной версии, нужно разрабатывать его также под IOS и Android  |
| PostgreSQL    | Database             | Реляционная база, дающая возможность масштабироваться за счет своего обширного функционала. Одна из лучших среди реляционных баз          |
| Redis         | Database             | In-memory база, хранящая данные в оперативной памяти, за счет чего ускоряет доступ к информации. Также предоставляет возможным шардирование и репликацию, что немало важно для высоконагруженного сервиса          |
| Amazon S3     | Cloud storage        | Одно из самых популярных и надежных хранилищ объектов, которое предоставляет масштабируемое, высокодоступное и безопасное хранение данных в облаке |
| Nginx         | Web-server           | Быстро отдает статику, поддерживает функционал кэширование и является отличным балансировщиком нагрузки |
| GitLab CI/CD  | CI/CD                | Обладает широким функционалом, включая интеграцию с Kubernetes, и обеспечивает интегрированную среду разработки (IDE) и контроль версий |
| Kubernetis    | Managment            | Является удобным для управления кластерами. Имеет множество преимуществ: легкое масшатбирование, эффективная отказоустойчивость, удобное управление приложениями и ресурсами |
| Docker        | Coneinerization      | Удобное развертование, изолированное окружение для каждого приложения, безопасность, а также удобство в использовании |
| Prometheus    | Monitoring           | Мониторинг производительности, быстрый сбор метрик, масштабируемость, доступная интеграция с Kubernetis |
| Grafana       | Visualizong          | Большой функционал для визуализации собранных метрик |
| gRPC          | RPC                  | Протокол удаленного вызова процедур, подходящих для большого списка нужд, с помощью которого с легкостью можно осуществить взаимодействие микросервисов          |
| RabitMQ       | Message queue        | Брокер очередей для асинхронной обработки запроса в случаях, когда быстрый ответ пользователю не нужен |

## Схема проекта

![Схема проекта](/img/architecture_1.png)

### Описание

Первоначально запросы поступают от пользователей через DNS-сервер, который по алгоритму Round-robin будет перенапрвлять на один из серверов-балансировщиков. Каждый из них будет перенапрявлять запрос на наименее загруженный сервер-бэкенд. У каждого сервера будет свой кэш запросов для быстрого ответа в случае популярного запроса.

Перейдем к кластеру API Getway, которые будут обрабатывать запросы, выполнять бизнес логику и ходить дальше в микросервисы за нужными данными. Весь кластер будет управляться Kubernetes. Все метрики будут собираться в базе Prometheus. У бэкендов будут свои балансировщики нагрузки, задача которых будет заключаться в балансировке нагрузки между узлами микросервисов.

Рассмотрим микросервисы:

Каждый микросервис выполенен в виде кластера, чтобы также распределять нагрузку. Общение бэкендов с микросервисами будет осуществялться с помощью протокола gRPS.

1. Authorization microservice

    Этот микросервис будет отвечать за сессии пользователей. Как говорилось выше, он будет ходить в кластер Редиса. Сам редис будет реплицировать данные для поддержания отказоустойчивости и быстро возвращать сессии нужному узлу микросервиса. Микросервис будет определять к какому шарду нужно будет обратиться.

    Все микросервисы будут управляться Kubernetes, метрики отправляются в свой кластер Prometheus. У каждой базы тоже будет свой сборщик метрик. Все метрики в последствии будут отпрвляться в Grafana.

2. Users microservice

    Будет хоить в кластер базы Постгреса, особенности работы которой были описаны выше. На основе range-based шардирования определит, в какой узел ему идти. Так же он будет взаимодействовать с микросервисом `Attachments` для получения, создания аватарки. Между этими двумя микросервисами будет прослойка в виде RebitMQ. Это позволит разгрузить трафик за счет ассинхронности.

3. Posts microservice

    Также ходит в Постгрес. В зависимости от запроса будет обращаться к шарду по принципу времени создания или по ранжированию. Также, как и в предыдущем пункте будет использоваться RebitMQ из-за слишком частого взаимодействия с микросервисом.

4. Attachment microservice

    Ходит в кластер S3-хранилища. Для более быстрой работы будет настроена контентная адресация, позволяящая определить, к какому узлу обратиться. Внутри него в базе Редиса будет проверяться кэш. Если в кэше ничего нет, ищем информацию в храниоище S3.

### Логика формирования ленты

Так как загрузка ленты является самым большим по rps запросу, стоит продумать, как она будет формироваться. Опишу следующую логику в архитектуре:

1. Когда пользователь запрашивает свою ленту, запрос сначала попадает на API Gateway.

2. API Gateway проверяет авторизацию пользователя, используя микросервис Authorization. Если пользователь не авторизован, возвращается ошибка.

3. После успешной авторизации API Gateway отправляет запрос на микросервис Users для получения подписчиков (или другой информации, в зависимости от алгоритма сбора ленты). Данные возвращаются на слой Getway.

4. Далее запрос с данными о нужных пользователях идет в микросервис Posts.Запрос может быть сформулирован на основе подписок пользователя или других более сложных алгоритмов (например, популярные посты пользователей, на которые подписан человек).

5. Микросервис Posts получает нужных пользователей, посты которых нужно подгрузить, проверяет, являются ли они новыми и в этом случае ищет именно новые посты.

6. В случае отсутствия новых проверяет кэш самых популярных запросов (часто запрашиваемых пользователями) и подгружает их вместе с постами, которые пользователь уже видел.

7. Для получения самого контента после составления плана подгрузки постов микросервис Posts идет в Attachments, где дальше ищется контент в S3-хранилище.

8. Если есть новые посты, микросервис Posts сохраняет их в свой кэш для будущих запросов.

9. Микросервис Posts возвращает посты в API Gateway.

10. API Gateway возвращает посты пользователю в качестве ответа на запрос ленты.

### Логический план запроса

Опишу логический план запроса для каждого типа запроса с клиента.

1. Запрос на создание нового поста

    - Клиент отправляет запрос на создание нового поста через API Gateway.
    - API Gateway перенаправляет запрос к микросервису Attachments.
    - Далее идем в S3 хранилище и ставим запрос в брокер очередей.
    - После сохранения изображения возвращаемся в Getway и идем в Posts.
    - Микросервис Posts сохраняет информацию о посте и формирует ответ с идентификатором созданного поста и отправляет его обратно клиенту через API Gateway.

2. Запрос на получение информации о пользователе

    - Клиент отправляет запрос на получение информации о пользователе через API Gateway.
    - API Gateway перенаправляет запрос к микросервису Users.
    - Микросервис Users получает информацию о пользователе из базы данных с учетом шардирования.
    - Микросервис Users формирует ответ с информацией о пользователе и отправляет его обратно клиенту через API Gateway.

3. Запрос на получение списка подписок/подписчиков пользователя

    - Клиент отправляет запрос на получение списка подписок или подписчиков пользователя через API Gateway.
    - API Gateway перенаправляет запрос к микросервису Users.
    - Микросервис Users получает информацию о подписках/подписчиках пользователя из базы данных с учетом шардирования.
    - Микросервис Users формирует ответ со списком подписок/подписчиков и отправляет его обратно клиенту через API Gateway.

4. Запрос на поиск пользователя

    - Клиент отправляет запрос на поиск пользователя через API Gateway, указывая критерии поиска (например, имя пользователя, электронная почта).
    - API Gateway перенаправляет запрос к микросервису Users.
    - Микросервис Users выполняет поиск по указанным критериям в базе данных с учетом шардирования.
    - Микросервис Users формирует ответ с результатами поиска (список найденных пользователей) и отправляет его обратно клиенту через API Gateway.

## Список серверов

<!-- 
--   Расчет емкости CDN
--   Расчет терминации SSL  
-->

<!-- -- Расчет нагрузки на хранилища, бенчмарки компонент в расчете серверов будут присутствовать -->

Пройдемся по каждому компоненту и определим, какая нагрузка будет на каждом из них. На основе этого определим, соклько и каких серверов нужно иметь.

### CDN сервера

Для более быстрого доступа к контенту будем использовать CDN сервера. Выделим список стран и кол-во серверов, где их будем размещать исходя из ананлиза аудитории:

- США (3 сервера)
- Индия (3 сервера)
- Бразилия (1 сервер)
- Япония (1 сервер)
- Австралия (1 сервер)
- Турция (1 сервер)
- Германия (1 сервер)
- Британия (1 сервер)

### Балансировщики

На основе [бенчмарков](https://www.linkedin.com/pulse/requests-per-second-how-much-cpuram-required-rishi-kumar), проделанных над nginx, можно расчитать, сколько балансировщиков будет использоваться. Стоит учитывать, что тафик будет поступать по протоколу HTTPS. Для обработки SSL соединений со средним размером 4.5 Мб. (подгрузка ленты из 15 постов по 300 Кб.) оптимально использовать 32-ядерный процессор. Как видно по таблице из представленного источника, при увеличении размера отдаваемого файла, рпс уменьшается экспоненциально. С нашими входными данными RPS будет равен 4 492. Теперь посчитаем, сколько серверов понадобится:

> 987 748 / 4 492 = 219 серверов

Для расчета сетевой емкости воспользуюсь посчитанными нагрузками на сеть: на пике имеем 3 784 Гбит/с. При 219 серверах 3 784 / 219 = 17 Гбит/с. Берем с запасом и получаем, что сетевая емкость каждого сервера будет 32 Гб. Также выделим 64 Гб RAM.

Также потребуется SSD на 128 Гб. для подгрузки статики и кэширования запросов.

### Бэкенды (API Getway)

Задача getway заключается в прокладки пути к микросервисам на основе входящего запроса. Сам бэкенд написан на Go с использованием многопотчности. Нужно выдерживать 987 748 RPS в сумме.

Так как запросы будут распределяться приблизительно равномерно, на бэкенды будет приходиьтся 1.5 доля от балансировщиков с учетом того, что какие-то бэкенды могут отказать и нагрузка будет перераспределяться. Тогда нам понадобится 273 сервера. Каждый из них должен выдерживать 987 748 / 383 = 2 578 rps. С такой задачей должен справиться 32 ядерный процессор. Также потребуется 64 Гбайт RAM для обработки задач.

Сетевой трафик для каждой машины - 3 784 / 383 = 9 Гбит/с. Сетевую карту беру с запасом, так как нагрузка может переаспределяться в случае отказа проблемных узлов - 16 Гб. емкостью.

<!-- Имеем формулы для подсчета CPU и RAM, основываясь на RPS, Task time, Worker memroy:

> **RPS** = **CPU** x (1 / **Task time**) `=>` \
**CPU** = **RPS** x **Task time**
>
> **RPS** = (**RAM** / **Worker memory**) x (1 / **Task time**) `=>` \
**RAM** = **Worker memory** x **Task time** x **RPS**

В качестве Task time возьмем 200 мс. (является средним значением), а Worker Memory возьмем 50 Mb. Посчитаем суммарные характеристики:

> CPU = 987 748 x 0.2 = 197 549
>
> RAM = 0.2 x 0.05 x 987 748 = 9 877 Gb

Для того, чтобы разнести это на части, возьмем максимальное количество ядер на одном сервере - 128 и получим:

> 197 549 / 128 = 15 434 сервера

где в каждом из них будет `9 877 / 128 = 77 Gb RAM`

Возьмем также 30 CPU, но 256 Гб RAM на каждый сервер, так как у приложений будет более сложная логика в отличие от nginx. Стоит учесть, что серверы будут ходить в микросервисы для получения нужной информации. Наличие большого SSD здесь не принципиально, возьмем 128 Гб. -->

### Микросервисы

1. Auth microservice

    Имеем 11 574 rps на пике. Микросервис будет ходить и обрабатывать данные сессий из Redis. Предположим, что микросервис сессий выполняет простые операции чтения и записи в Redis и имеет небольшую вычислительную сложность. При таком раскладе сервер может иметь task time равный 15 мс. Считаем коичество ядер:

    > **RPS** = **CPU** x (1 / **Task time**) `=>` \
    > **CPU** = **RPS** x **Task time**
    >
    > 11 574 x 0.15 = 1 736 CPU

    Хватит 64 CPU. Всего понадобится `11 574 / 64 = 27` серверов. Так как много оперативной памяти не потребуется, выделим 8 Гб. на каждый. При стевой нагрузке 176 Гб/с понадобится карта с объемом 8 Гб.

2. Users microservice

    В этот компонент будут попадать следующие значимые запросы: поиск пользователей, поиск подписок/подписчиков, получение рользователя по id и т. д. Стоит учесть, что каждый запрос на пост сопровождается походом в users для получения автора поста. Из этого сделаем вывод, что нагрузка на пользователей будет больше, чем на посты, так как будут учитываться дополнительно поиск пользователя, переход на него из поста. Таким образом все типы запросов кроме авторизации будут использовать этот микросервис. Так как уже было посчитано кол-во серверов для обработки всех запросов, возьмем 95% от них - `273 x 0.95 = 259` серверов с 32 CPU, 64 Гб. RAM и 16 Гб. сетевой емкости.

3. Posts microservice

    Считаем запросы, которые связаны только с постами: просмотр ленты, проставление лайков, оставление комментариев, публикация фото/видео - 950 000 rps. Таким образом выделяем при учете того, что 32 ядра смогут обрабатывать 5 000 запросов в секунду, 950 000 / 5 000 = 190 серверов.

    Выделяем 32 Gb RAM, 64 Gb сетевой емкости

4. Attachments microservice

    Этот компонент будет одним из главным, так как здесь будет находиться работа со всем контентом сервиса. Из запросов сюда будут входить подгрузка ленты, создание аватарки, загрузка контента. Исходя из пиковых нагрузок, сервис должен держать 879 628 + 2 198 = 880 000 rps. Возьмем долю 0.88 от серверов, котрые держат все запросы - `273 x 0.88 = 240` серверов. Возьмем с дополнительно еще 30, так как это узкое место, работающее с большими данными. Получим 270 серверов. Также выделим 32 CPU и 64 Gb RAM.

    На загрузку приходится 31 Гбит/с трафика, на подгрузку постов 3 546 Гбит/с. Тогда для поддержания стабильного трафика нужно около 16 Гб. сетевой емкости. SSD берем так же на 16 Гб.

### Базы данных

#### Postgres

- Users

    По расчетам емкости в предыдущих пунктах было получено, что на хранение пользователей уходит 8 000 Тб. Из них без автарки (то есть для хранения в реляционной базе) `8 000 000 - 50 x 2 000 000 000 = 8000 - 100 = 7 900 Тб.`

    Итого, для хранения пользователей понадобится 7 900 Тб. Для хранения такого объема возьмем, что один сервер будет хранить 288 Тб (2 диска по 144 Тб.). В довесок возьмем 10 SSD карт по 4 Тб. для быстрого доступа к популярному контенту. Также берем 32 CPU и 128 RAM.

    Всего серверов понадобится: `2 900 / 228 = 27`

    Нагрузка на базы пользователей будут 3 784 / 27 = 140 Гбит/с. Сетевая карта будет объемом 256 Гб.

    На запись будут идти запросы регистрации 108 000 / (24 * 3600) ~ 5 rps, следовательно достаточно сделать одну базу Master, а остальные запросы будут на чтение, пожтому 26 реплик.

- Subscriptions

    Будем счиать, что в среднем у пользователя будет 100 подписчиков и 50 подписок. Мы лишь усредняем значение, так как у каких-то аккаунтов может быть 50 подписчиков, а у других и вовсе 2 миллиона. Каждая подписка весит 16 байт - 2 id пользователя

    Для подписчиков:

    > 2 000 000 000 x 100 x 16 = 3 200 Гб = 3.2 Тб.

    Для подписок это число в два раза меньше 1.6 Тб.

    При 5 Тб. можно взять SSD диски по 500 Гб. Выделяем 10 серверов. Из них 3 на мастер и 7 на реплики, так как чтение будет происходить намного чаще, чем запись.

- Posts

    Всего на посты тратится 58.5 Пб. места. Из 58 Пб. не вложений будут 50 000 000 000 х 6 Кбайт (описание) = 300 Тб.

    Выделим в одном сервере 4 SSD диска по 500 Гб. Итого будет 150 серверов для хранения данных.

    На запись будет идти 5% запросов, поэтому под мастер выделим 7 серверов, а на чтение 143.

- Likes

    Лайков всего на 800 Тб. Будет очень частая запись и более редкое чтение. Имеет смысл взять 10 SSD диска на 4 Тб. каждый и 2 HHD на 64 Тб. Выделяем 20 серверов.

    10 серверов будут на запись - Мастер, 10 реплик.

- Comments

    Лайков на 2.5 Пб. Будет редкая запись и частое чтение. Берем 2 HHD в сервер на 144 Тб. и 10 SSD на 4 Тб. Выделяем 10 серверов, 3 из которых будут на запись, а 7 на чтение.

#### Redis

Так как нам нужно хранить 500 000 000 сессий в день, запросов будет поступать 11 000 в секунду. Redis хранит все в оперативной памяти. Возьмем 256 Гб. RAM, SSD диск на 500 Гб, 32 CPU. Один такой сервер сможет справится с нагрузкой в 1 000 rps. Всего понадобится 11 серверов.

#### S3

##### Фото

Размер фотографий = 0.0003 х 50 000 000 000 х 0.75 = 11 250 Тб. = 11.25 Пб

Максимальный размер бакета будет 5 Тб. Тогда всего бакетов:

> 11 250 / 5 = 2250 бакетов

##### Аватарки

Размер аватарки = 100 Кб. х 2 000 000 000 х 0.8 = 160 Тб. Размер бакетов:

> 160 / 5 = 32 бакета

##### Видео

Размер видео = 58.5 - 11.25 = 47.25 Пб. Размер бакетов:

> 47 250 / 5 = 9 450 бакетов

Резюмируя, изображу полученные данные в таблице:

| Кластер                  | Кол-во серверов    | CPU             | RAM, Гб         | Диск, Гб        |
|------------------------- | ------------------ | --------------- | --------------- | --------------- |
| Nginx                    | 219                | 32              | 64              | 128 (SSD)       |
| Getway                   | 273                | 32              | 64              | 128 (SSD)       |
| Auth microservice        | 27                 | 32              | 8               | 128 (SSD)       |
| Users microservice       | 259                | 32              | 64              | 128 (SSD)       |
| Posts microservice       | 270                | 32              | 64              | 128 (SSD)       |
| Attachments microservice | 240                | 32              | 64              | 128 (SSD)       |
| Users PostgreSQL         | 27                 | 64              | 128             | 2 по 144 Тб (HHD), 10 по 2 Тб. (SSD)   |
| Followers PostgreSQL     | 10                 | 32              | 64              | 1 по 500 Гб. (SSD)   |
| Posts PostgreSQL         | 150                | 64              | 128             | 4 по 500 Гб. (SSD)   |
| Likes PostgreSQL         | 20                | 64              | 128             | 2 по 64 Тб. (HHD), 10 по 4 Тб. (SSD)   |
| Comments PostgreSQL      | 10                | 64              | 128             | 2 по 144 Тб. (HHD), 10 по 4 Тб. (SSD)   |
| S3                       | 100                | 30              | 128             | Фото:  |
| Redis                    | 11                  | 32               | 256             | 500 Гб (SSD)       |

## Источники

1. [Статистика по пользователям](https://www.statista.com/topics/1882/instagram)
2. [Количество пользователей в месяц](https://www.statista.com/statistics/183585/instagram-number-of-global-users/)
3. [Количесвто пользователей в день](https://www.businessdit.com/time-spend-instagram-statistics/)
4. [Лайки и комментарии](https://mention.com/en/blog/instagram-statistics-report/)
5. [Среднее время проведенное пользователем](https://www.businessdit.com/time-spend-instagram-statistics/)
6. [Битрейт видео](https://www.puckermob.com/tech/everything-you-wanted-to-know-about-best-bitrate-for-instagram/)
7. [Количество постов](https://earthweb.com/how-many-pictures-are-on-instagram/)
8. [Nginx нагрузка](https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/)
